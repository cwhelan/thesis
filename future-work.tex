\chapter{Future Work}\label{chap_future_work}

In this thesis we have demonstrated that distributed computing, in the form of Hadoop and MapReduce, can be applied to the SV detection problem, enabling highly accurate algorithms with very fast runtimes. In addition, we showed that the use of discriminative sequence labeling techniques, in the form of conditional random fields, can be used to integrate many different signals of SVs and by doing so can improve Cloudbreak's results. We hope that the work presented in this thesis is not only useful in itself but is extensible enough to contribute to additional innovations in the detection of structural variations.

\section{Cloudbreak}

There are many possible extensions that could be made to enhance Cloudbreak's effectiveness as a general SV analysis tool for high-throughput sequencing data. These include:

\begin{itemize}
 \item \emph{Support the detection of additional SV types such as longer deletions, inversions, and translocations}. These would be useful and necessary additions to a complete variant detection pipeline. We hope that the abstract formulation of our MapReduce framework in Chapter~\ref{chap_framework} will create a base from which to extend Cloudbreak's capabilities and implement many additional SV detection algorithms in the future. For example, detection of small to medium-sized inversions could be realized by adding the orientation of mapped read pairs as an additional component of the mixtures used in Cloudbreak's feature generation function. One difficulty in this effort will be the relative lack of validated SVs that are not deletions and inversion to develop and test against, but as large scale projects such as the 1000 Genomes continue to generate data this concern should be mitigated.
 \item \emph{Extend Cloudbreak's GMM-based feature generation function to relax the assumption of a diploid genome sample}. Currently Cloudbreak forces a GMM with two components. This assumption would not apply for some sequencing projects, especially in cancer-related research. In a heterogeneous tumor sample, for example, there may be multiple tumor sub-clones, as well as normal DNA, in the sequenced sample, and each set of input DNA might have different genotypes. With additional modeling of these mixtures, it might be possible for Cloudbreak not only to detect different variants in a sample but also to estimate their relative abundances, something that would be useful for tracking tumor clone evolution.
 \item \emph{Add a local assembly step to increase breakpoint resolution and validate SV candidates.} Cloudbreak's breakpoint resolution is lower than that of many other RP algorithms. Although we improved that result in Chapter~\ref{chap_crf}, we could not obtain single nucleotide resolution, because our features were defined in terms of fixed-size genomic windows. As we mentioned in Chapter~\ref{chap_related_work}, one approach to improving breakpoint resolution, and providing additional validation of predicted results, is to attempt to conduct a \emph{de novo} assembly of the reads that mapped near the breakpoints as well as their paired sequences. If effectively implemented, this would be a very useful addition to any SV detection algorithm. Theoretically, each breakpoint could be assembled simultaneously in reduce tasks in an additional MapReduce job. This would greatly increase the confidence and utility of Cloudbreak's predictions.
 \item \emph{Support the simultaneous analysis of multiple samples and libraries.} Approaches such as Genome STRiP~\cite{Handsaker:2011ki} have demonstrated that they can achieve higher accuracy than single-sample SV detection algorithms by simultaneously considering many samples at once. Even if a variant is only supported by a very low number of reads in each individual sample, by pooling data from multiple samples the presence of a variant can become clear. Additional modeling in Cloudbreak of multiple samples, each with its own insert size distribution, might provide a similar increase in power. Alternatively, data could be pooled in Cloudbreak and the resulting predictions could be used as features in our discriminative machine learning models.
 \item \emph{Adopt emerging standardized Hadoop libraries and file formats.} As more effort is put into developing Hadoop-based sequencing analysis applications, it will become increasingly important that these applications use consistent APIs and data formats so that they can be composed into large pipelines. Once they contain the requisite features, refactoring Cloudbreak to use libraries such as Hadoop-BAM~\cite{Niemenmaa:2012hu} would be a step in that direction.
\end{itemize}

\section{SV Detection with Discriminative Machine Learning}

We believe that the application of CRFs to SV detection we described in Chapter~\ref{chap_crf} is very promising and could be extended in many directions:

\begin{itemize}
\item \emph{Construction of improved training data sets.} The most obvious limiting factor for this project was the training and test data used. Training on simulated data ignores many potential sources of noise as well as possible signals that could be used to detect SVs. Training on real data sets, such as those that are being generated for the 1000 Genomes project, would allow much greater potential for learning.
\item \emph{Using additional or different sets of labels.} In some sequence labeling tasks, it can improve performance to model the start and end of variants with separate labels, or to use different types of labels for flanking sequence. It would be useful to test different labeling schemes to determine the optimal one for this task. In addition, the authors of forestSV~\cite{Michaelson:2012fj} demonstrated some utility in directly labeling regions known to cause false positives for other algorithms in training their classifier; this might be useful in our model as well.
\item \emph{Add additional features to the model.} There are many possible additional features that could be used in our model. One example would be possible SNVs near the breakpoint locations, indicated by mismatches between the reads and the reference. There are also many other possible genome annotations or sources of prior knowledge that could be used as features, including the locations of previously identified variants in other samples. 
\item \emph{Learning for additional variant types.} As mentioned above, it would be useful to train our model to detect additional variant types by expanding the label set and set of features used
\item \emph{Alternative learning methods.} Recently, many advances have been made in machine learning for sequencing labeling tasks through the use of deep learning using artificial neural networks. With additional data to train on, it may be possible to apply these techniques to the SV detection problem.
\todo{\item Integrate generation of training and testing data seamlessly into the Cloudbreak workflow.}
\end{itemize}

\section{Gibbon Genome Breakpoint Analysis}

The identification of enrichment of genomic features at the location of gibbon-human synteny breakpoints can be used as background for further research into the evolutionary history of gibbons, with an ultimate goal of understanding the processes that caused them:

\begin{itemize}
\item \emph{Analysis of epigenetic marks associated with breakpoints.} The Carbone lab has been investigating epigenetic signatures near genomic breakpoints, and has found distinct patterns of DNA methylation in breakpoint-associated transposable elements~\cite{Lazar:2014}. DNA methylation may have a role in the formation or conservation of rearranged genomes. Further analysis may reveal new mechanisms of gibbon genome rearrangement.
\item \emph{Exploration of association of large DNA structures and domains with breakpoints.} The association of CTCF binding sites with gibbon breakpoints suggests a possible relationship between rearrangement sites and the three dimensional structure of DNA. Alternatively, CTCF's role as separator of regulatory domains may have rendered genome rearrangements that did not respect those boundaries unviable, causing only those present in today's gibbons to be preserved. Additional analysis of other types of data may allow us to gain insight into these questions.
\end{itemize}